{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# LSCC lab meeting\n",
      "\n",
      "## 2014.03.21\n",
      "## Juan Nunez-Iglesias\n",
      "\n",
      "Live update on high-content screen clustering... Other projects if time allows."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Introduction\n",
      "\n",
      "Explanation of HCS and why it matters to us"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As always, we start by setting matplotlib inline and importing pyplot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Next, we set the working directory..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "working_dir = '~/Data/marcelle-hcs-sample/features'\n",
      "os.chdir(os.path.expanduser(working_dir))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Set up file mapping\n",
      "\n",
      "The working directory contains a pre-populated file called `dirs.txt` that contains all the directories on the merri cluster in which a feature map was created. (As well as the RGB images.) (This was done simply with an `ls -d` command on merri.) We use this to copy over each feature matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import paramiko as mk\n",
      "\n",
      "with open('dirs.txt', 'r') as fin:\n",
      "    line = fin.readline().rstrip()\n",
      "    dirs = line.split(' ')\n",
      "\n",
      "from husc.screens import myofusion as myo\n",
      "\n",
      "remote_base_dir = 'marcelle/raw-data'\n",
      "plateids = map(myo.dir2plate, dirs)\n",
      "feature_fns = ['features-%i.h5' % p for p in plateids]\n",
      "\n",
      "# set up SFTP transport to fetch files\n",
      "trans = mk.Transport(('merri.vlsci.unimelb.edu.au', 22))\n",
      "trans.connect(pkey=mk.Agent()._keys[0], username='jni')\n",
      "sftp = mk.SFTPClient.from_transport(trans)\n",
      "\n",
      "#for d, f in zip(dirs, feature_fns):\n",
      "#    if not os.path.exists(f):\n",
      "#        sftp.get(os.path.join(remote_base_dir, d, 'features.h5'), f)\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's do a sanity check to ensure plate ids are unique:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (len(dirs), len(set(dirs)),\n",
      "       len(plateids), len(set(plateids)))\n",
      "tempdict = {}\n",
      "for k in plateids:\n",
      "    tempdict.setdefault(k, []).append(k)\n",
      "print([k for k, v in tempdict.items() if len(v) > 1])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That repeated \"0\" is because of the \"NOCODE\" plates above. We ignore them and plow bravely on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Data input\n",
      "\n",
      "Next, we read in all feature maps using `pandas`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h5_fns = sorted(filter(lambda s: s.startswith('feature-') and\n",
      "                       s.endswith('.h5') and not\n",
      "                       s.endswith('NOCODE.h5'), os.listdir('.')))\n",
      "datasets = [pd.read_hdf(f, 'data') for f in h5_fns]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.concat(datasets)\n",
      "print(data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "(This is a reduced dataset for demo --- full dataset has 45,000 rows!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Initial exploratory analysis\n",
      "\n",
      "Now, let's use scikit-learn to do some PCA.\n",
      "\n",
      "**Warning: the following code is exploratory. Still figuring out best practices. Suggestions are welcome.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import decomposition"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = StandardScaler().fit_transform(data)\n",
      "pca = decomposition.PCA().fit(X)\n",
      "pca.n_components = 2\n",
      "X_reduced = pca.fit_transform(X)\n",
      "print(X_reduced.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The obligatory PCA scatter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(*(X_reduced.T))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This is *not* what a PCA scatter normally looks like. The assumptions of normality are clearly violated. What can I do about this?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What features contributed most to the two top PCA components?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca_comp1, pca_comp2 = (np.argsort(np.abs(pca.components_[0]))[-1:-4:-1],\n",
      "                        np.argsort(np.abs(pca.components_[1]))[-1:-4:-1])\n",
      "data.columns[pca_comp1], data.columns[pca_comp2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "And how much were these contributions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(pca.components_[0][pca_comp1] / (pca.components_[0]).sum(),\n",
      " pca.components_[1][pca_comp2] / (pca.components_[1]).sum())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Clustering\n",
      "\n",
      "Let's cluster the data (using KMeans):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# average cluster size of 10\n",
      "avg_cluster_size = 20\n",
      "K = cluster.MiniBatchKMeans(n_clusters=(X.shape[0] // avg_cluster_size))\n",
      "K = K.fit(X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(minibatch KMeans is an online clustering algorithm that performs KMeans on subsets of the data, adding subsets sequentially.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Mapping data rows to genes and images\n",
      "\n",
      "Now, we can find out which images clustered together and which interfering RNAs they correspond to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from husc import preprocess as pre\n",
      "from husc.screens import myofusion as myo\n",
      "myores_semantic_filename = myo.myores_semantic_filename"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sems = map(myo.myores_semantic_filename, data.index)\n",
      "data.index = [(s['plate'], s['well']) for s in sems]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plates = set([ix[0] for ix in data.index])\n",
      "print plates"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Mapping files to genes\n",
      "\n",
      "The gene data has been pre-loaded into a MongoDB database. It maps all plates and wells to genes and images. (See `husc.screens.myofusion.populate_db`.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymongo import MongoClient\n",
      "collection = MongoClient()[\"myofusion\"][\"wells\"]\n",
      "\n",
      "def get_cluster_gene_data(cluster_id):\n",
      "    cluster_members = [data.index[i] for\n",
      "                       i in np.flatnonzero(K.labels_ == cluster_id)]\n",
      "    cluster_entries = [collection.find_one({\"_id\": myo.key2mongo(c)})\n",
      "                       for c in cluster_members]\n",
      "    return cluster_members, cluster_entries"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Notes:\n",
      "\n",
      " - `collection.find_one` returns a \"`Document`\". Later we will see `collection.find`, which returns a \"`Cursor`\", which is a lazy iterable collection of matching documents.\n",
      " \n",
      " - Documents are simple {key: value} collections. Valid JSON objects are supported. Missing keys allowed. Only restriction is `_id` key must exist and be unique for each document. (This can be done automatically by MongoDB.)\n",
      " \n",
      " - MongoDB does not support tuples as keys, so I wrote a simple function to convert a `(plate, well)` tuple to a string."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Cluster sizes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to examine some clusters and we want moderately-sized ones, so let's plot the cluster sizes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(len(K.labels_), len(np.unique(K.labels_)), np.max(K.labels_))\n",
      "plt.plot(np.bincount(K.labels_))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Some example clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_members_1, cluster_entries_1 = get_cluster_gene_data(39)\n",
      "print(len(cluster_members_1))\n",
      "cluster_entries_1[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can use the plate and well numbers to grab the images that were analysed. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Grabbing the files from merri"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The database contains the locations of each file in merri. We use our previously established paramiko connection (held by the `sftp` object) to grab files on request."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tempfile as tmp\n",
      "import mahotas as mh\n",
      "\n",
      "def get_doc_value(mongo_doc, key):\n",
      "    try:\n",
      "        return mongo_doc[key], False\n",
      "    except KeyError:\n",
      "        return \"\", True\n",
      "\n",
      "def get_images(key, key_type='_id',\n",
      "               ids_returned=[\"_id\", \"gene_name\"],\n",
      "               failsafe_ids=[\"label\"]):\n",
      "    if key_type == \"_id\" and type(key) == tuple:\n",
      "        key = myo.key2mongo(key)\n",
      "    im_entries = collection.find({key_type: key})\n",
      "    ims = []\n",
      "    ids = []\n",
      "    for entry in im_entries:\n",
      "        im_path = entry[\"filename\"]\n",
      "        tmpfile = tmp.mkstemp(suffix='.' + im_path.split('.')[-1],\n",
      "                              dir='.')[1]\n",
      "        sftp.get(im_path, tmpfile)\n",
      "        ims.append(mh.imread(tmpfile))\n",
      "        id_list = []\n",
      "        fail = False\n",
      "        for ix in ids_returned:\n",
      "            val, fail = get_doc_value(entry, ix)\n",
      "            id_list.append(val)\n",
      "        if fail:\n",
      "            for ix in failsafe_ids:\n",
      "                id_list.append(get_doc_value(entry, ix)[0])\n",
      "        ids.append(\" \".join(map(str, id_list)))\n",
      "    labeled_ims = zip(ids, ims)\n",
      "    return labeled_ims"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now we want to look at the images. The below function shows images in an n x 3 rectangle."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_3ims(label_ims):\n",
      "    num_rows = int(np.ceil(float(len(label_ims)) / 3))\n",
      "    fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))\n",
      "    for (name, im), ax in zip(label_ims, axes.ravel()):\n",
      "        ax.imshow(im)\n",
      "        ax.set_title(name)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_members_1"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    ims = [get_images(k)[0] for k in cluster_members_1]\n",
      "except:\n",
      "    print([c[\"filename\"] for c in cluster_entries_1])\n",
      "    raise"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_3ims(ims)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Another example\n",
      "\n",
      "The images look quite similar! Is this a fluke? Let's look at another cluster."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plate_cluster_2, gene_cluster_2 = get_cluster_gene_data(40)\n",
      "plate_cluster_2"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    ims2 = [get_images(k)[0] for k in plate_cluster_2]\n",
      "except:\n",
      "    print(c[\"filename\"] for c in gene_cluster_2)\n",
      "    raise"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_3ims(ims2)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Retrieving specific genes\n",
      "\n",
      "Let's look at some images previously shown to be of interest to Christophe and his collaborator Bruno, such as Sass6, Son, and Cenpq."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools as it\n",
      "\n",
      "ims3 = list(it.chain(*[get_images(g, key_type=\"gene_name\") for\n",
      "                       g in [\"Sass6\", \"Son\", \"Cenpq\"]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show_3ims(ims3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# To be continued...\n",
      "\n",
      "## clustering best practices\n",
      "\n",
      " - PCA/KMeans/Euclidean distance is almost certainly the wrong algorithm here\n",
      " - Will try hierarchical clustering/correlation next. Suggestions *very* welcome!\n",
      " - Need to account for plate/well bias. Can define a linear model, then what? What normalisation to use here?\n",
      "\n",
      "## some sanity checks\n",
      "\n",
      "These need to be checked systematically:\n",
      "\n",
      " - distance between feature vectors of same gene should be much closer than random pairs\n",
      " - distance between positive controls and between negative controls should be much closer than between pairs of the two\n",
      "\n",
      "## Clustering of entire dataset\n",
      "\n",
      " - rather large population of image artifacts... Not sure how to deal with them. [but maybe plate normalisation will help!]\n",
      "\n",
      "## Gene Ontology enrichment of clusters\n",
      "\n",
      " - Associate specific GO functions with the clusters\n",
      " \n",
      "## Interactivity\n",
      "\n",
      " - \"stretch goal\" of this project: ship a web server with each screen coming out of VCFG\n",
      " - interactively look at images in each cluster: zoom, toggle exposure corrections, mask/isolate channels, etc\n",
      " - plot of features for each image; click on individual feature gives histogram of feature for full dataset compared to this image\n",
      " - navigate by clicking/zooming/panning on MDS plot; relevant data (image, feature map, gene name, GO, cluster info) appear on adjacent panel; other images from same gene highlighted\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}